import grpc
from proto import model_service_pb2
from proto import model_service_pb2_grpc
import sys
import os
sys.path.append('.')

def test_llm_direct():
    """ç›´æŽ¥æ¸¬è©¦ LLM API æœå‹™"""
    print("ðŸ¤– ç›´æŽ¥æ¸¬è©¦ LLM API æœå‹™")
    print("=" * 50)
    
    try:
        # å°Žå…¥ä¸¦åˆå§‹åŒ–æœå‹™
        from apis.llm_service import LLMServicer
        
        print("ðŸ”§ åˆå§‹åŒ– LLM æœå‹™...")
        servicer = LLMServicer(model_name="microsoft/DialoGPT-medium")  # ä½¿ç”¨å°æ¨¡åž‹æ¸¬è©¦
        
        # å‰µå»ºå‡çš„ä¸Šä¸‹æ–‡
        class FakeContext:
            def __init__(self):
                self.error_code = None
                self.error_details = None
            
            def set_code(self, code): 
                self.error_code = code
                print(f"âŒ éŒ¯èª¤ä»£ç¢¼: {code}")
            
            def set_details(self, details): 
                self.error_details = details
                print(f"âŒ éŒ¯èª¤è©³æƒ…: {details}")
        
        context = FakeContext()
        
        # æ¸¬è©¦ 1: æ–‡æœ¬ç”Ÿæˆ
        print("\nðŸ“ æ¸¬è©¦æ–‡æœ¬ç”Ÿæˆ...")
        text_request = model_service_pb2.TextGenerationRequest(
            prompt="The future of artificial intelligence is",
            max_tokens=50,
            temperature=0.7,
            top_p=0.9
        )
        
        print(f"è«‹æ±‚: {text_request.prompt}")
        response = servicer.GenerateText(text_request, context)
        
        if response.success:
            print(f"âœ… ç”ŸæˆæˆåŠŸ:")
            print(f"ðŸ“„ çµæžœ: {response.generated_text}")
        else:
            print("âŒ ç”Ÿæˆå¤±æ•—")
            if context.error_details:
                print(f"éŒ¯èª¤: {context.error_details}")
        
        # æ¸¬è©¦ 2: å°è©±
        print("\nðŸ’¬ æ¸¬è©¦å°è©±...")
        chat_request = model_service_pb2.ChatCompletionRequest(
            messages=[
                model_service_pb2.ChatMessage(role="system", content="You are a helpful assistant."),
                model_service_pb2.ChatMessage(role="user", content="Hello! Can you tell me about AI?"),
            ],
            max_tokens=80,
            temperature=0.7
        )
        
        print("å°è©±è¨Šæ¯:")
        for msg in chat_request.messages:
            print(f"  {msg.role}: {msg.content}")
        
        chat_response = servicer.ChatCompletion(chat_request, context)
        
        if chat_response.success:
            print(f"âœ… å°è©±æˆåŠŸ:")
            print(f"ðŸ¤– å›žæ‡‰: {chat_response.response}")
        else:
            print("âŒ å°è©±å¤±æ•—")
            if context.error_details:
                print(f"éŒ¯èª¤: {context.error_details}")
        
        print("\nðŸŽ‰ ç›´æŽ¥æ¸¬è©¦å®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_llm_grpc():
    """é€éŽ gRPC æ¸¬è©¦ LLM æœå‹™"""
    print("ðŸŒ é€éŽ gRPC æ¸¬è©¦ LLM æœå‹™")
    print("=" * 50)
    
    try:
        # é€£æŽ¥åˆ° gRPC ä¼ºæœå™¨
        print("ðŸ”— é€£æŽ¥åˆ° gRPC ä¼ºæœå™¨...")
        with grpc.insecure_channel('localhost:50051') as channel:
            stub = model_service_pb2_grpc.MediaServiceStub(channel)
            
            # æ¸¬è©¦æ–‡æœ¬ç”Ÿæˆ
            print("\nðŸ“ ç™¼é€æ–‡æœ¬ç”Ÿæˆè«‹æ±‚...")
            request = model_service_pb2.TextGenerationRequest(
                prompt="äººå·¥æ™ºæ…§çš„æœªä¾†ç™¼å±•è¶¨å‹¢æ˜¯",
                max_tokens=60,
                temperature=0.8
            )
            
            print(f"è«‹æ±‚: {request.prompt}")
            response = stub.GenerateText(request)
            
            if response.success:
                print(f"âœ… æˆåŠŸ!")
                print(f"ðŸ“„ ç”Ÿæˆçµæžœ: {response.generated_text}")
            else:
                print("âŒ å¤±æ•—")
            
            # æ¸¬è©¦å°è©±
            print("\nðŸ’¬ ç™¼é€å°è©±è«‹æ±‚...")
            chat_request = model_service_pb2.ChatCompletionRequest(
                messages=[
                    model_service_pb2.ChatMessage(role="user", content="ä½ å¥½ï¼è«‹ç°¡å–®ä»‹ç´¹ä¸€ä¸‹ä½ è‡ªå·±ã€‚"),
                ],
                max_tokens=100,
                temperature=0.7
            )
            
            print("å°è©±è¨Šæ¯:")
            for msg in chat_request.messages:
                print(f"  {msg.role}: {msg.content}")
            
            chat_response = stub.ChatCompletion(chat_request)
            
            if chat_response.success:
                print(f"âœ… æˆåŠŸ!")
                print(f"ðŸ¤– å›žæ‡‰: {chat_response.response}")
            else:
                print("âŒ å¤±æ•—")
        
        print("\nðŸŽ‰ gRPC æ¸¬è©¦å®Œæˆï¼")
        return True
                
    except grpc.RpcError as e:
        print(f"âŒ gRPC éŒ¯èª¤: {e.code()} - {e.details()}")
        return False
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "grpc":
        test_llm_grpc()
    else:
        test_llm_direct()