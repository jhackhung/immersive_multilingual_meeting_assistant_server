import grpc
import logging
import sys
import os

# æ·»åŠ æ¨¡å‹è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from models.llm_service_model import LLMServicer as LLMModel
from proto import model_service_pb2, model_service_pb2_grpc

logger = logging.getLogger(__name__)

class LLMServicer(model_service_pb2_grpc.MediaServiceServicer):
    """LLM API æœå‹™å±¤"""
    
    def __init__(self, model_name="gpt2"):
        """
        åˆå§‹åŒ– LLM æœå‹™
        
        Args:
            model_name: ä½¿ç”¨çš„æ¨¡å‹åç¨±
            æ¨è–¦æ¨¡å‹:
            - "gpt2" (å¿«é€Ÿæ¸¬è©¦)
            - "microsoft/DialoGPT-medium" (å°è©±)
            - "facebook/blenderbot-400M-distill" (èŠå¤©)
        """
        self.model_name = model_name
        print(f"ğŸ¤– åˆå§‹åŒ– LLM API æœå‹™ - æ¨¡å‹: {model_name}")
        
        try:
            # åˆå§‹åŒ–åº•å±¤æ¨¡å‹
            self.llm_model = LLMModel(model_name=model_name)
            print("âœ… LLM API æœå‹™åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ LLM API æœå‹™åˆå§‹åŒ–å¤±æ•—: {e}")
            raise

    def GenerateText(self, request, context):
        """è™•ç†æ–‡æœ¬ç”Ÿæˆè«‹æ±‚"""
        try:
            logger.info(f"LLM API: æ”¶åˆ°æ–‡æœ¬ç”Ÿæˆè«‹æ±‚")
            logger.info(f"æç¤ºè©: {request.prompt[:100]}...")
            logger.info(f"åƒæ•¸: max_tokens={request.max_tokens}, temperature={request.temperature}")
            
            # å§”è¨—çµ¦æ¨¡å‹å±¤è™•ç†
            response = self.llm_model.GenerateText(request, context)
            
            if response.success:
                logger.info(f"LLM API: æ–‡æœ¬ç”ŸæˆæˆåŠŸï¼Œé•·åº¦: {len(response.generated_text)}")
            else:
                logger.error("LLM API: æ–‡æœ¬ç”Ÿæˆå¤±æ•—")
            
            return response
            
        except Exception as e:
            error_msg = f"LLM API æ–‡æœ¬ç”ŸæˆéŒ¯èª¤: {str(e)}"
            logger.error(error_msg)
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(error_msg)
            return model_service_pb2.TextGenerationResponse(
                generated_text="",
                success=False
            )

    def ChatCompletion(self, request, context):
        """è™•ç†å°è©±å®Œæˆè«‹æ±‚"""
        try:
            logger.info(f"LLM API: æ”¶åˆ°å°è©±å®Œæˆè«‹æ±‚")
            logger.info(f"è¨Šæ¯æ•¸é‡: {len(request.messages)}")
            
            # è¨˜éŒ„å°è©±å…§å®¹
            for i, message in enumerate(request.messages):
                logger.info(f"è¨Šæ¯ {i+1}: {message.role} - {message.content[:50]}...")
            
            # å§”è¨—çµ¦æ¨¡å‹å±¤è™•ç†
            response = self.llm_model.ChatCompletion(request, context)
            
            if response.success:
                logger.info(f"LLM API: å°è©±å®ŒæˆæˆåŠŸ")
                logger.info(f"å›æ‡‰: {response.response[:100]}...")
            else:
                logger.error("LLM API: å°è©±å®Œæˆå¤±æ•—")
            
            return response
            
        except Exception as e:
            error_msg = f"LLM API å°è©±å®ŒæˆéŒ¯èª¤: {str(e)}"
            logger.error(error_msg)
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(error_msg)
            return model_service_pb2.ChatCompletionResponse(
                response="",
                success=False
            )