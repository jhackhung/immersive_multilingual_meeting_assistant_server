"""
StreamingDiarization: è™•ç†å³æ™‚å‚³å…¥çš„éŸ³è¨Šæµ (ä¸²æµè™•ç†)
é‡å°é«˜é€š NPU å„ªåŒ–çš„ç‰ˆæœ¬

å¿…è¦å®‰è£çš„å‡½å¼åº«:
pip install numpy
pip install onnxruntime
pip install scikit-learn
pip install librosa
"""

import os
import numpy as np
import onnxruntime as ort
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_similarity
from scipy.spatial.distance import cdist
import librosa
import time
from typing import List, Tuple, Optional
import logging

class StreamingDiarization:
    def __init__(self, 
                 segmentation_model_path: str = "models/pyannote_segmentation_static.onnx", 
                 embedding_model_path: str = "models/pyannote_embedding_static.onnx", 
                 device: str = 'cpu',
                 use_qualcomm_npu: bool = False):
        """
        åˆå§‹åŒ–é‡å°é«˜é€š NPU å„ªåŒ–çš„å³æ™‚ä¸²æµèªè€…è¾¨è­˜å™¨ã€‚

        Args:
            segmentation_model_path (str): VAD/åˆ†å‰²æ¨¡å‹ (.onnx) çš„è·¯å¾‘
            embedding_model_path (str): è²ç´‹åµŒå…¥æ¨¡å‹ (.onnx) çš„è·¯å¾‘
            device (str): 'cpu', 'cuda', æˆ– 'qnn' (é«˜é€š NPU)
            use_qualcomm_npu (bool): æ˜¯å¦ä½¿ç”¨é«˜é€š NPU åŠ é€Ÿ
        """
        print("æ­£åœ¨åˆå§‹åŒ–é«˜é€šå„ªåŒ–çš„ StreamingDiarization...")
        
        # æª¢æŸ¥æ¨¡å‹æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        if not os.path.exists(segmentation_model_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°åˆ†å‰²æ¨¡å‹: {segmentation_model_path}")
        if not os.path.exists(embedding_model_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°åµŒå…¥æ¨¡å‹: {embedding_model_path}")
        
        # è¨­å®šåŸ·è¡Œæä¾›è€…
        if use_qualcomm_npu and device == 'qnn':
            # é«˜é€š NPU åŸ·è¡Œæä¾›è€…
            providers = ['QNNExecutionProvider', 'CPUExecutionProvider']
            provider_options = [
                {
                    'backend_path': 'QnnHtp.dll',  # Windows
                    # 'backend_path': 'libQnnHtp.so',  # Linux/Android
                    'profiling_level': 'basic'
                },
                {}
            ]
        elif device.lower() == 'cuda':
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
            provider_options = None
        else:
            providers = ['CPUExecutionProvider']
            provider_options = None

        # è¼‰å…¥æ¨¡å‹
        try:
            if provider_options:
                self.seg_session = ort.InferenceSession(
                    segmentation_model_path, 
                    providers=providers,
                    provider_options=provider_options
                )
                self.emb_session = ort.InferenceSession(
                    embedding_model_path, 
                    providers=providers,
                    provider_options=provider_options
                )
            else:
                self.seg_session = ort.InferenceSession(segmentation_model_path, providers=providers)
                self.emb_session = ort.InferenceSession(embedding_model_path, providers=providers)
                
            print(f"âœ… æ¨¡å‹å·²è¼‰å…¥ï¼Œä½¿ç”¨åŸ·è¡Œæä¾›è€…: {providers[0]}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise
        
        # ç²å–æ¨¡å‹è¼¸å…¥/è¼¸å‡ºè³‡è¨Š
        self.seg_input_name = self.seg_session.get_inputs()[0].name
        self.seg_input_shape = self.seg_session.get_inputs()[0].shape
        self.seg_output_shape = self.seg_session.get_outputs()[0].shape
        
        self.emb_input_name = self.emb_session.get_inputs()[0].name
        self.emb_input_shape = self.emb_session.get_inputs()[0].shape
        self.emb_output_shape = self.emb_session.get_outputs()[0].shape
        
        print(f"åˆ†å‰²æ¨¡å‹: è¼¸å…¥ {self.seg_input_shape}, è¼¸å‡º {self.seg_output_shape}")
        print(f"åµŒå…¥æ¨¡å‹: è¼¸å…¥ {self.emb_input_shape}, è¼¸å‡º {self.emb_output_shape}")

        # éŸ³è¨Šè™•ç†åƒæ•¸ (é‡å°ç§»å‹•è£ç½®å„ªåŒ–)
        self.sample_rate = 16000
        self.chunk_seconds = 0.5
        self.chunk_samples = int(self.chunk_seconds * self.sample_rate)
        self.buffer_seconds = 5.0
        self.buffer_samples = int(self.buffer_seconds * self.sample_rate)
        
        # æ¨¡å‹è¼¸å…¥é•·åº¦ (å¾ ONNX æ¨¡å‹ç²å–)
        self.seg_required_length = self.seg_input_shape[1] if len(self.seg_input_shape) > 1 else self.chunk_samples
        self.emb_required_length = self.emb_input_shape[1] if len(self.emb_input_shape) > 1 else int(1.5 * self.sample_rate)
        
        # ç‹€æ…‹ç®¡ç†
        self.audio_buffer = np.array([], dtype=np.float32)
        self.speech_in_progress = False
        self.speech_start_sample = 0
        self.total_samples_processed = 0

        # å„ªåŒ–çš„èšé¡åƒæ•¸ (é©åˆå³æ™‚è™•ç†)
        self.speaker_embeddings = []  # æ¯å€‹èªªè©±è€…çš„åµŒå…¥é›†åˆ
        self.speaker_centroids = []   # æ¯å€‹èªªè©±è€…çš„ä¸­å¿ƒé»
        self.clustering_threshold = 0.7  # é¤˜å¼¦ç›¸ä¼¼åº¦é–¾å€¼
        self.min_segment_duration = 0.3  # æœ€å°ç‰‡æ®µé•·åº¦
        self.max_speakers = 10  # æœ€å¤§èªªè©±è€…æ•¸é‡é™åˆ¶
        
        # æ•ˆèƒ½å„ªåŒ–
        self.enable_voice_activity_detection = True
        self.vad_threshold_high = 0.6  # èªéŸ³é–‹å§‹é–¾å€¼  
        self.vad_threshold_low = 0.3   # èªéŸ³çµæŸé–¾å€¼
        
        print("âœ… é«˜é€šå„ªåŒ–ä¸²æµè¾¨è­˜å™¨å·²å°±ç·’")

    def process(self, audio_chunk: np.ndarray) -> List[Tuple[str, float, float]]:
        """
        è™•ç†ä¸€å°æ®µå‚³å…¥çš„éŸ³è¨Š (é‡å°å³æ™‚æ€§å„ªåŒ–)

        Args:
            audio_chunk (np.ndarray): éŸ³è¨Šæ•¸æ“š (float32, å–®è²é“)

        Returns:
            list: æ–°ç¢ºå®šçš„ (speaker_label, start_time, end_time) å…ƒçµ„åˆ—è¡¨
        """
        if not isinstance(audio_chunk, np.ndarray):
            audio_chunk = np.array(audio_chunk, dtype=np.float32)

        # ç¢ºä¿éŸ³è¨Šæ˜¯å–®è²é“ä¸”æ­£ç¢ºæ ¼å¼
        if len(audio_chunk.shape) > 1:
            audio_chunk = np.mean(audio_chunk, axis=1)
        
        # æ›´æ–°ç·©è¡å€
        self.audio_buffer = np.concatenate([self.audio_buffer, audio_chunk])

        newly_finalized_segments = []
        
        # ç•¶ç·©è¡å€æœ‰è¶³å¤ æ•¸æ“šæ™‚é€²è¡Œè™•ç†
        if len(self.audio_buffer) >= self.seg_required_length:
            
            # 1. èªéŸ³æ´»å‹•æª¢æ¸¬ (VAD)
            if self.enable_voice_activity_detection:
                speech_prob = self._detect_speech_activity()
                
                # 2. ç‹€æ…‹æ©Ÿè™•ç†
                segments = self._handle_speech_state_machine(speech_prob, len(audio_chunk))
                newly_finalized_segments.extend(segments)
            
            # 3. æ›´æ–°ç‹€æ…‹
            self.total_samples_processed += len(audio_chunk)
            
            # 4. ç¶­è­·ç·©è¡å€å¤§å° (è¨˜æ†¶é«”ç®¡ç†)
            if len(self.audio_buffer) > self.buffer_samples:
                self.audio_buffer = self.audio_buffer[-self.buffer_samples:]

        return newly_finalized_segments

    def _detect_speech_activity(self) -> float:
        """
        ä½¿ç”¨ ONNX VAD æ¨¡å‹æª¢æ¸¬èªéŸ³æ´»å‹•
        """
        try:
            # æº–å‚™è¼¸å…¥æ•¸æ“š
            input_audio = self._prepare_vad_input()
            
            # ONNX æ¨ç†
            ort_inputs = {self.seg_input_name: input_audio}
            ort_outputs = self.seg_session.run(None, ort_inputs)
            
            # è§£æè¼¸å‡º (å‡è¨­è¼¸å‡ºæ ¼å¼ç‚º [batch, time, classes] æˆ– [batch, classes])
            output = ort_outputs[0]
            
            if len(output.shape) == 3:
                # [batch, time, classes] - å– speech é¡åˆ¥çš„å¹³å‡æ©Ÿç‡
                speech_prob = np.mean(output[0, :, 1])  # å‡è¨­ index 1 æ˜¯ speech
            elif len(output.shape) == 2:
                # [batch, classes] - ç›´æ¥å– speech é¡åˆ¥
                speech_prob = output[0, 1]
            else:
                # [batch] - ç›´æ¥ä½¿ç”¨è¼¸å‡ºå€¼
                speech_prob = output[0]
            
            return float(speech_prob)
            
        except Exception as e:
            logging.error(f"VAD æ¨ç†éŒ¯èª¤: {e}")
            return 0.0

    def _prepare_vad_input(self) -> np.ndarray:
        """æº–å‚™ VAD æ¨¡å‹çš„è¼¸å…¥"""
        # å–æœ€æ–°çš„éŸ³è¨Šç‰‡æ®µ
        if len(self.audio_buffer) >= self.seg_required_length:
            audio_segment = self.audio_buffer[-self.seg_required_length:]
        else:
            # ä¸è¶³å‰‡è£œé›¶
            audio_segment = np.zeros(self.seg_required_length, dtype=np.float32)
            audio_segment[:len(self.audio_buffer)] = self.audio_buffer
        
        # æ¨™æº–åŒ–
        if np.std(audio_segment) > 0:
            audio_segment = (audio_segment - np.mean(audio_segment)) / np.std(audio_segment)
        
        # è½‰æ›ç‚ºæ¨¡å‹æœŸæœ›çš„æ ¼å¼ [batch, samples] æˆ– [batch, 1, samples]
        if len(self.seg_input_shape) == 3:
            return np.expand_dims(np.expand_dims(audio_segment, axis=0), axis=1).astype(np.float32)
        else:
            return np.expand_dims(audio_segment, axis=0).astype(np.float32)

    def _handle_speech_state_machine(self, speech_prob: float, chunk_size: int) -> List[Tuple[str, float, float]]:
        """è™•ç†èªéŸ³æª¢æ¸¬çš„ç‹€æ…‹æ©Ÿ"""
        segments = []
        
        # èªéŸ³é–‹å§‹æª¢æ¸¬
        if speech_prob > self.vad_threshold_high and not self.speech_in_progress:
            self.speech_in_progress = True
            self.speech_start_sample = self.total_samples_processed
            
        # èªéŸ³çµæŸæª¢æ¸¬  
        elif speech_prob < self.vad_threshold_low and self.speech_in_progress:
            self.speech_in_progress = False
            segment_end_sample = self.total_samples_processed + chunk_size
            
            # æª¢æŸ¥ç‰‡æ®µé•·åº¦
            duration = (segment_end_sample - self.speech_start_sample) / self.sample_rate
            if duration >= self.min_segment_duration:
                
                # æå–ä¸¦è™•ç†é€™æ®µèªéŸ³
                segment = self._process_speech_segment(segment_end_sample)
                if segment:
                    segments.append(segment)
        
        return segments

    def _process_speech_segment(self, end_sample: int) -> Optional[Tuple[str, float, float]]:
        """è™•ç†ä¸€å€‹å®Œæ•´çš„èªéŸ³ç‰‡æ®µ"""
        try:
            # å¾ç·©è¡å€æå–èªéŸ³ç‰‡æ®µ
            start_in_buffer = max(0, self.speech_start_sample - 
                                (self.total_samples_processed - len(self.audio_buffer)))
            end_in_buffer = len(self.audio_buffer)
            
            speech_audio = self.audio_buffer[start_in_buffer:end_in_buffer]
            
            if len(speech_audio) < self.sample_rate * 0.1:  # å¤ªçŸ­å‰‡å¿½ç•¥
                return None
            
            # æå–è²ç´‹åµŒå…¥
            embedding = self._extract_speaker_embedding(speech_audio)
            if embedding is None:
                return None
            
            # åˆ†é…èªªè©±è€…
            speaker_id = self._assign_speaker(embedding)
            
            # å»ºç«‹çµæœ
            speaker_label = f"SPEAKER_{speaker_id:02d}"
            start_time = self.speech_start_sample / self.sample_rate
            end_time = end_sample / self.sample_rate
            
            return (speaker_label, start_time, end_time)
            
        except Exception as e:
            logging.error(f"èªéŸ³ç‰‡æ®µè™•ç†éŒ¯èª¤: {e}")
            return None

    def _extract_speaker_embedding(self, speech_audio: np.ndarray) -> Optional[np.ndarray]:
        """
        ä½¿ç”¨ ONNX åµŒå…¥æ¨¡å‹æå–èªªè©±è€…ç‰¹å¾µ
        """
        try:
            # æº–å‚™è¼¸å…¥ (ç¢ºä¿é•·åº¦ç¬¦åˆæ¨¡å‹è¦æ±‚)
            if len(speech_audio) < self.emb_required_length:
                # é‡è¤‡å¡«å……
                repeat_factor = int(np.ceil(self.emb_required_length / len(speech_audio)))
                speech_audio = np.tile(speech_audio, repeat_factor)[:self.emb_required_length]
            elif len(speech_audio) > self.emb_required_length:
                # å–ä¸­é–“éƒ¨åˆ†
                mid = len(speech_audio) // 2
                half_len = self.emb_required_length // 2
                speech_audio = speech_audio[mid - half_len:mid + half_len]
            
            # éŸ³è¨Šé è™•ç†
            if np.std(speech_audio) > 0:
                speech_audio = (speech_audio - np.mean(speech_audio)) / np.std(speech_audio)
            
            # æº–å‚™ ONNX è¼¸å…¥
            if len(self.emb_input_shape) == 3:
                onnx_input = np.expand_dims(np.expand_dims(speech_audio, axis=0), axis=1).astype(np.float32)
            else:
                onnx_input = np.expand_dims(speech_audio, axis=0).astype(np.float32)
            
            # æ¨ç†
            ort_inputs = {self.emb_input_name: onnx_input}
            ort_outputs = self.emb_session.run(None, ort_inputs)
            
            # æå–åµŒå…¥å‘é‡ (é€šå¸¸æ˜¯æœ€å¾Œä¸€å€‹è¼¸å‡ºçš„ç¬¬ä¸€å€‹æ‰¹æ¬¡)
            embedding = ort_outputs[0][0]
            
            # L2 æ­£è¦åŒ–
            norm = np.linalg.norm(embedding)
            if norm > 0:
                embedding = embedding / norm
            
            return embedding
            
        except Exception as e:
            logging.error(f"åµŒå…¥æå–éŒ¯èª¤: {e}")
            return None

    def _assign_speaker(self, embedding: np.ndarray) -> int:
        """
        åˆ†é…èªªè©±è€… ID (å„ªåŒ–çš„ç·šä¸Šèšé¡)
        """
        if len(self.speaker_centroids) == 0:
            # ç¬¬ä¸€å€‹èªªè©±è€…
            self.speaker_embeddings.append([embedding])
            self.speaker_centroids.append(embedding.copy())
            return 0
        
        # è¨ˆç®—èˆ‡ç¾æœ‰èªªè©±è€…çš„ç›¸ä¼¼åº¦
        similarities = []
        for centroid in self.speaker_centroids:
            similarity = cosine_similarity([embedding], [centroid])[0][0]
            similarities.append(similarity)
        
        max_similarity = max(similarities)
        best_speaker_idx = similarities.index(max_similarity)
        
        # åˆ¤æ–·æ˜¯å¦åˆ†é…çµ¦ç¾æœ‰èªªè©±è€…æˆ–å»ºç«‹æ–°èªªè©±è€…
        if max_similarity > self.clustering_threshold:
            # åˆ†é…çµ¦ç¾æœ‰èªªè©±è€…
            self.speaker_embeddings[best_speaker_idx].append(embedding)
            
            # æ›´æ–°è©²èªªè©±è€…çš„ä¸­å¿ƒé» (ç§»å‹•å¹³å‡)
            alpha = 0.1  # å­¸ç¿’ç‡
            self.speaker_centroids[best_speaker_idx] = (
                (1 - alpha) * self.speaker_centroids[best_speaker_idx] + 
                alpha * embedding
            )
            
            return best_speaker_idx
        else:
            # å»ºç«‹æ–°èªªè©±è€… (å¦‚æœæœªé”åˆ°æœ€å¤§æ•¸é‡é™åˆ¶)
            if len(self.speaker_centroids) < self.max_speakers:
                self.speaker_embeddings.append([embedding])
                self.speaker_centroids.append(embedding.copy())
                return len(self.speaker_centroids) - 1
            else:
                # é”åˆ°é™åˆ¶ï¼Œåˆ†é…çµ¦æœ€ç›¸ä¼¼çš„ç¾æœ‰èªªè©±è€…
                return best_speaker_idx

    def get_statistics(self) -> dict:
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        return {
            "total_speakers": len(self.speaker_centroids),
            "total_processed_time": self.total_samples_processed / self.sample_rate,
            "buffer_size": len(self.audio_buffer),
            "speech_in_progress": self.speech_in_progress
        }

    def reset(self):
        """é‡ç½®æ‰€æœ‰ç‹€æ…‹"""
        self.audio_buffer = np.array([], dtype=np.float32)
        self.speech_in_progress = False
        self.speech_start_sample = 0
        self.total_samples_processed = 0
        self.speaker_embeddings = []
        self.speaker_centroids = []
        print("âœ… StreamingDiarization å·²é‡ç½®")

# è¼”åŠ©å‡½æ•¸ï¼šæ¨¡å‹é©—è­‰
def validate_onnx_models(seg_model_path: str = "models/pyannote_segmentation_static.onnx", 
                        emb_model_path: str = "models/pyannote_embedding_static.onnx"):
    """é©—è­‰ ONNX æ¨¡å‹æ˜¯å¦æ­£ç¢ºè¼‰å…¥"""
    try:
        print("ğŸ” é©—è­‰æ¨¡å‹...")
        
        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        if not os.path.exists(seg_model_path):
            print(f"âŒ æ‰¾ä¸åˆ°åˆ†å‰²æ¨¡å‹: {seg_model_path}")
            return False
        if not os.path.exists(emb_model_path):
            print(f"âŒ æ‰¾ä¸åˆ°åµŒå…¥æ¨¡å‹: {emb_model_path}")
            return False
        
        # è¼‰å…¥æ¨¡å‹
        seg_session = ort.InferenceSession(seg_model_path)
        emb_session = ort.InferenceSession(emb_model_path)
        
        print(f"âœ… åˆ†å‰²æ¨¡å‹: {seg_session.get_inputs()[0].shape}")
        print(f"âœ… åµŒå…¥æ¨¡å‹: {emb_session.get_inputs()[0].shape}")
        
        # æ¸¬è©¦æ¨ç†
        seg_input_shape = seg_session.get_inputs()[0].shape
        emb_input_shape = emb_session.get_inputs()[0].shape
        
        # å»ºç«‹æ¸¬è©¦è¼¸å…¥
        if len(seg_input_shape) == 3:
            test_seg_input = np.random.randn(1, 1, seg_input_shape[2]).astype(np.float32)
        else:
            test_seg_input = np.random.randn(1, seg_input_shape[1]).astype(np.float32)
            
        if len(emb_input_shape) == 3:
            test_emb_input = np.random.randn(1, 1, emb_input_shape[2]).astype(np.float32)
        else:
            test_emb_input = np.random.randn(1, emb_input_shape[1]).astype(np.float32)
        
        # æ¸¬è©¦æ¨ç†
        seg_output = seg_session.run(None, {seg_session.get_inputs()[0].name: test_seg_input})
        emb_output = emb_session.run(None, {emb_session.get_inputs()[0].name: test_emb_input})
        
        print(f"âœ… åˆ†å‰²æ¨¡å‹è¼¸å‡º: {seg_output[0].shape}")
        print(f"âœ… åµŒå…¥æ¨¡å‹è¼¸å‡º: {emb_output[0].shape}")
        print("ğŸ‰ æ¨¡å‹é©—è­‰æˆåŠŸï¼")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹é©—è­‰å¤±æ•—: {e}")
        return False
