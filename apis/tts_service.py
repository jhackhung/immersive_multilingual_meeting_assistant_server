import grpc
import torch
import numpy as np
import os
import time
import onnxruntime as ort
import io
import tempfile
from scipy.io.wavfile import write as write_wav

from proto import model_service_pb2
from proto import model_service_pb2_grpc

from TTS.api import TTS

print("ğŸš€ æ­£åœ¨åˆå§‹åŒ– TTS æœå‹™...")
FIXED_MEL_CHUNK_LENGTH = 100
DEFAULT_SPEAKER_WAV_PATH = "./tts_sample/segment.wav"
ONNX_MODEL_PATH = "../model/hifigan.onnx"
SAMPLE_RATE = 22050

if not os.path.exists(DEFAULT_SPEAKER_WAV_PATH):
    raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°é è¨­åƒè€ƒéŸ³è¨Šæª”æ¡ˆ: {DEFAULT_SPEAKER_WAV_PATH}ã€‚")
if not os.path.exists(ONNX_MODEL_PATH):
    raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°å„ªåŒ–å¾Œçš„ ONNX æ¨¡å‹: {ONNX_MODEL_PATH}ã€‚")

# --- è¨­å®šé‹ç®—è¨­å‚™ ---
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"âœ… TTS æœå‹™ä½¿ç”¨è£ç½®: {device}")

# --- è¼‰å…¥ XTTS-v2 æ¨¡å‹ ---
print("â³ æ­£åœ¨è¼‰å…¥ XTTS-v2 æ¨¡å‹...")
tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to(device)
tts_model = tts.synthesizer.tts_model
print("âœ… XTTS-v2 æ¨¡å‹è¼‰å…¥æˆåŠŸã€‚")

# --- è¼‰å…¥å„ªåŒ–å¾Œçš„ ONNX è²ç¢¼å™¨æ¨¡å‹ ---
print("â³ æ­£åœ¨è¼‰å…¥å„ªåŒ–å¾Œçš„ ONNX è²ç¢¼å™¨...")

if device == 'cuda':
    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
else:
    providers = ['CPUExecutionProvider']
    
onnx_session = ort.InferenceSession(ONNX_MODEL_PATH, providers=providers)
print(f"âœ… ONNX è²ç¢¼å™¨è¼‰å…¥æˆåŠŸï¼Œä½¿ç”¨ provider: {onnx_session.get_providers()}")

print("âœ… TTS æœå‹™åˆå§‹åŒ–å®Œæˆã€‚")

class TtsServicer(model_service_pb2_grpc.TtsServiceServicer):
    """
    å¯¦ç¾ tts.proto ä¸­å®šç¾©çš„ TtsServiceã€‚
    """
    def Tts(self, request, context):
        """
        è™•ç† TTS è«‹æ±‚ä¸¦å›å‚³ç”Ÿæˆçš„éŸ³è¨Šã€‚
        'request' æ˜¯ä¸€å€‹ tts_pb2.TtsRequest ç‰©ä»¶ã€‚
        å¿…é ˆå›å‚³ä¸€å€‹ tts_pb2.TtsResponse ç‰©ä»¶ã€‚
        """
        start_time = time.time()
        speaker_wav_path = DEFAULT_SPEAKER_WAV_PATH
        temp_file_path = None

        try:
            # --- æ­¥é©Ÿ 1: è™•ç†åƒè€ƒéŸ³è¨Š ---
            # gRPC ç›´æ¥å‚³è¼¸ bytesï¼Œå¦‚æœæä¾›äº† reference_audioï¼Œæˆ‘å€‘å°‡å…¶å¯«å…¥æš«å­˜æª”
            if request.reference_audio:
                with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
                    tmp_file.write(request.reference_audio)
                    temp_file_path = tmp_file.name
                    speaker_wav_path = temp_file_path
                print("ğŸ¤ ä½¿ç”¨äº†å®¢æˆ¶ç«¯æä¾›çš„åƒè€ƒéŸ³è¨Šã€‚")
            else:
                print(f"ğŸ¤ ä½¿ç”¨é è¨­çš„åƒè€ƒéŸ³è¨Š: {DEFAULT_SPEAKER_WAV_PATH}")

            # --- æ­¥é©Ÿ 2: ç”Ÿæˆæ¢…çˆ¾é »è­œ (æ ¸å¿ƒé‚è¼¯èˆ‡ä¹‹å‰ç›¸åŒ) ---
            text_to_speak = request.text_to_speak
            print(f"ğŸ“ æº–å‚™ç”Ÿæˆæ–‡å­—: '{text_to_speak[:30]}...'")
            language = request.language if request.language else "en"
            gpt_cond_latent, speaker_embedding = tts_model.get_conditioning_latents(audio_path=speaker_wav_path)
            gpt_cond_latent = gpt_cond_latent.to(device)
            speaker_embedding = speaker_embedding.to(device)

            text_tokens = torch.IntTensor(tts_model.tokenizer.encode(text_to_speak, lang=language)).unsqueeze(0).to(device)

            with torch.no_grad():
                gpt_codes = tts_model.gpt.generate(
                    cond_latents=gpt_cond_latent, text_inputs=text_tokens,
                    output_attentions=False, temperature=0.75, top_p=0.9,
                    repetition_penalty=5.0, length_penalty=1.5
                )
                expected_output_len = torch.tensor([gpt_codes.shape[-1] * tts_model.gpt.code_stride_len], device=device)
                text_len = torch.tensor([text_tokens.shape[-1]], device=device)
                gpt_latents = tts_model.gpt(
                    text_tokens, text_len, gpt_codes, expected_output_len,
                    cond_latents=gpt_cond_latent, return_attentions=False, return_latent=True
                )
            
            mel_spectrogram_tensor = gpt_latents.detach()

            # --- æ­¥é©Ÿ 3: ä½¿ç”¨ ONNX è²ç¢¼å™¨é€²è¡Œåˆ†å¡Šæ¨è«– ---
            full_mel_spectrogram_np = mel_spectrogram_tensor.cpu().numpy().astype(np.float32)
            speaker_embedding_np = speaker_embedding.cpu().numpy().astype(np.float32)

            final_audio_waveform = []
            num_chunks = (full_mel_spectrogram_np.shape[1] + FIXED_MEL_CHUNK_LENGTH - 1) // FIXED_MEL_CHUNK_LENGTH
            
            for i in range(num_chunks):
                start_idx = i * FIXED_MEL_CHUNK_LENGTH
                end_idx = min((i + 1) * FIXED_MEL_CHUNK_LENGTH, full_mel_spectrogram_np.shape[1])
                mel_chunk = full_mel_spectrogram_np[:, start_idx:end_idx, :]
                current_chunk_length = mel_chunk.shape[1]

                if current_chunk_length < FIXED_MEL_CHUNK_LENGTH:
                    padding_needed = FIXED_MEL_CHUNK_LENGTH - current_chunk_length
                    mel_chunk_padded = np.pad(mel_chunk, ((0, 0), (0, padding_needed), (0, 0)), mode='constant', constant_values=0)
                else:
                    mel_chunk_padded = mel_chunk

                onnx_inputs = {"mel_spectrogram": mel_chunk_padded, "speaker_embedding": speaker_embedding_np}
                chunk_output = onnx_session.run(None, onnx_inputs)[0]
                final_audio_waveform.append(chunk_output.squeeze())

            # --- æ­¥é©Ÿ 4: æ‹¼æ¥éŸ³è¨Šä¸¦è½‰æ›ç‚º bytes ---
            if not final_audio_waveform:
                context.set_code(grpc.StatusCode.INTERNAL)
                context.set_details("éŸ³è¨Šç”Ÿæˆå¤±æ•—ï¼Œæ²’æœ‰ä»»ä½•éŸ³è¨Šå¡Šç”¢ç”Ÿã€‚")
                return model_service_pb2.TtsResponse()

            final_audio_waveform = np.concatenate(final_audio_waveform)
            
            buffer = io.BytesIO()
            write_wav(buffer, SAMPLE_RATE, final_audio_waveform.astype(np.float32))
            wav_bytes = buffer.getvalue()
            
            end_time = time.time()
            print(f"âœ… è«‹æ±‚è™•ç†å®Œæˆï¼Œç¸½è€—æ™‚: {end_time - start_time:.2f} ç§’ã€‚")
            
            # --- æ­¥é©Ÿ 5: å›å‚³ gRPC å›æ‡‰ ---
            return model_service_pb2.TtsResponse(generated_audio=wav_bytes)

        except Exception as e:
            print(f"âŒ è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(f"å…§éƒ¨ä¼ºæœå™¨éŒ¯èª¤: {str(e)}")
            return model_service_pb2.TtsResponse()
        
        finally:
            # --- æ¸…ç†æš«å­˜æª”æ¡ˆ ---
            if temp_file_path and os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
                print(f"ğŸ—‘ï¸ å·²åˆªé™¤æš«å­˜æª”æ¡ˆ: {temp_file_path}")