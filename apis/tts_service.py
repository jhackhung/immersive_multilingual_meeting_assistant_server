import grpc
import torch
import numpy as np
import os
import time
import onnxruntime as ort
import io
import tempfile
from scipy.io.wavfile import write as write_wav

from proto import model_service_pb2
from proto import model_service_pb2_grpc

from TTS.api import TTS

class TtsServicer(model_service_pb2_grpc.MediaServiceServicer):
    """
    å¯¦ç¾ .proto ä¸­å®šç¾©çš„ TtsServiceã€‚
    é€™å€‹é¡åˆ¥å°‡æ¨¡å‹è¼‰å…¥å’Œæ¨è«–é‚è¼¯å°è£åœ¨ä¸€èµ·ã€‚
    """
    
    def __init__(self, 
                 onnx_model_path="./models/hifigan_decoder.onnx", 
                 default_speaker_wav="./tts_sample/segment.wav"):
        """
        åˆå§‹åŒ– TtsServicerï¼Œè¼‰å…¥æ‰€æœ‰å¿…è¦çš„æ¨¡å‹å’Œè¨­å®šã€‚
        é€™å€‹æ–¹æ³•åªæœƒåœ¨ä¼ºæœå™¨å•Ÿå‹•ã€å»ºç«‹æ­¤é¡åˆ¥å¯¦ä¾‹æ™‚åŸ·è¡Œä¸€æ¬¡ã€‚
        
        Args:
            onnx_model_path (str): ONNX è²ç¢¼å™¨æ¨¡å‹çš„è·¯å¾‘ã€‚
            default_speaker_wav (str): é è¨­åƒè€ƒéŸ³è¨Šçš„è·¯å¾‘ã€‚
        """

        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ– TTS æœå‹™...")
        self.onnx_model_path = onnx_model_path
        self.default_speaker_wav_path = default_speaker_wav
        self.sample_rate = 22050
        self.fixed_mel_chunk_length = 100

        if not os.path.exists(self.default_speaker_wav_path):
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°é è¨­åƒè€ƒéŸ³è¨Šæª”æ¡ˆ: {self.default_speaker_wav_path}")
        if not os.path.exists(self.onnx_model_path):
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°å„ªåŒ–å¾Œçš„ ONNX æ¨¡å‹: {self.onnx_model_path}")

        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"âœ… TTS æœå‹™ä½¿ç”¨è£ç½®: {self.device}")    

        # --- è¼‰å…¥ XTTS-v2 æ¨¡å‹ ---
        print("â³ æ­£åœ¨è¼‰å…¥ XTTS-v2 æ¨¡å‹...")
        tts_instance = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to(self.device)
        self.tts_model = tts_instance.synthesizer.tts_model
        print("âœ… XTTS-v2 æ¨¡å‹è¼‰å…¥æˆåŠŸã€‚")

        # --- è¼‰å…¥å„ªåŒ–å¾Œçš„ ONNX è²ç¢¼å™¨æ¨¡å‹ ---
        print("â³ æ­£åœ¨è¼‰å…¥å„ªåŒ–å¾Œçš„ ONNX è²ç¢¼å™¨...")
        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if self.device == 'cuda' else ['CPUExecutionProvider']
        try:
            self.onnx_session = ort.InferenceSession(self.onnx_model_path, providers=providers)
            print(f"âœ… ONNX è²ç¢¼å™¨è¼‰å…¥æˆåŠŸï¼Œä½¿ç”¨ provider: {self.onnx_session.get_providers()}")
        except Exception as e:
            raise RuntimeError(f"âŒ è¼‰å…¥ ONNX æ¨¡å‹å¤±æ•—: {e}")
        
        print("âœ… TTS æœå‹™åˆå§‹åŒ–å®Œæˆã€‚")

    def Tts(self, request, context):
        """
        è™•ç† TTS è«‹æ±‚ä¸¦å›å‚³ç”Ÿæˆçš„éŸ³è¨Šã€‚
        'request' æ˜¯ä¸€å€‹ tts_pb2.TtsRequest ç‰©ä»¶ã€‚
        å¿…é ˆå›å‚³ä¸€å€‹ tts_pb2.TtsResponse ç‰©ä»¶ã€‚
        """
        start_time = time.time()
        speaker_wav_path = self.default_speaker_wav_path
        temp_file_path = None

        try:
            # --- è™•ç†åƒè€ƒéŸ³è¨Š ---
            if request.reference_audio:
                with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
                    tmp_file.write(request.reference_audio)
                    temp_file_path = tmp_file.name
                    speaker_wav_path = temp_file_path
                print("ğŸ¤ ä½¿ç”¨äº†å®¢æˆ¶ç«¯æä¾›çš„åƒè€ƒéŸ³è¨Šã€‚")
            else:
                print(f"ğŸ¤ ä½¿ç”¨é è¨­çš„åƒè€ƒéŸ³è¨Š: {self.default_speaker_wav_path}")
                
            # --- ç”Ÿæˆæ¢…çˆ¾é »è­œ ---
            text_to_speak = request.text_to_speak
            language = request.language or "en"
            print(f"ğŸ“ æº–å‚™ç”Ÿæˆæ–‡å­— (èªè¨€: {language}): '{text_to_speak[:30]}...'")
            
            # ä½¿ç”¨ self.tts_model å’Œ self.device
            gpt_cond_latent, speaker_embedding = self.tts_model.get_conditioning_latents(audio_path=speaker_wav_path)
            gpt_cond_latent = gpt_cond_latent.to(self.device)
            speaker_embedding = speaker_embedding.to(self.device)

            text_tokens = torch.IntTensor(self.tts_model.tokenizer.encode(text_to_speak, lang=language)).unsqueeze(0).to(self.device)


            with torch.no_grad():
                gpt_codes = self.tts_model.gpt.generate(
                    cond_latents=gpt_cond_latent, text_inputs=text_tokens,
                    output_attentions=False, temperature=0.75, top_p=0.9,
                    repetition_penalty=5.0, length_penalty=1.5
                )
                expected_output_len = torch.tensor([gpt_codes.shape[-1] * self.tts_model.gpt.code_stride_len], device=self.device)
                text_len = torch.tensor([text_tokens.shape[-1]], device=self.device)
                gpt_latents = self.tts_model.gpt(
                    text_tokens, text_len, gpt_codes, expected_output_len,
                    cond_latents=gpt_cond_latent, return_attentions=False, return_latent=True
                )
            
            mel_spectrogram_tensor = gpt_latents.detach()

            # --- æ­¥é©Ÿ 3: ä½¿ç”¨ ONNX è²ç¢¼å™¨é€²è¡Œåˆ†å¡Šæ¨è«– ---
            full_mel_spectrogram_np = mel_spectrogram_tensor.cpu().numpy().astype(np.float32)
            speaker_embedding_np = speaker_embedding.cpu().numpy().astype(np.float32)

            final_audio_waveform = []
            num_chunks = (full_mel_spectrogram_np.shape[1] + self.fixed_mel_chunk_length - 1) // self.fixed_mel_chunk_length
            
            for i in range(num_chunks):
                start_idx = i * self.fixed_mel_chunk_length
                end_idx = min((i + 1) * self.fixed_mel_chunk_length, full_mel_spectrogram_np.shape[1])
                mel_chunk = full_mel_spectrogram_np[:, start_idx:end_idx, :]
                current_chunk_length = mel_chunk.shape[1]

                if current_chunk_length < self.fixed_mel_chunk_length:
                    padding_needed = self.fixed_mel_chunk_length - current_chunk_length
                    mel_chunk_padded = np.pad(mel_chunk, ((0, 0), (0, padding_needed), (0, 0)), mode='constant', constant_values=0)
                else:
                    mel_chunk_padded = mel_chunk

                onnx_inputs = {"mel_spectrogram": mel_chunk_padded, "speaker_embedding": speaker_embedding_np}
                # ä½¿ç”¨ self.onnx_session
                chunk_output = self.onnx_session.run(None, onnx_inputs)[0]
                final_audio_waveform.append(chunk_output.squeeze())

            # --- æ‹¼æ¥éŸ³è¨Šä¸¦è½‰æ›ç‚º bytes ---
            if not final_audio_waveform:
                context.set_code(grpc.StatusCode.INTERNAL)
                context.set_details("éŸ³è¨Šç”Ÿæˆå¤±æ•—ï¼Œæ²’æœ‰ä»»ä½•éŸ³è¨Šå¡Šç”¢ç”Ÿã€‚")
                return model_service_pb2.TtsResponse()

            final_audio_waveform = np.concatenate(final_audio_waveform)
            
            buffer = io.BytesIO()
            write_wav(buffer, self.sample_rate, final_audio_waveform.astype(np.float32))
            wav_bytes = buffer.getvalue()
            
            end_time = time.time()
            print(f"âœ… è«‹æ±‚è™•ç†å®Œæˆï¼Œç¸½è€—æ™‚: {end_time - start_time:.2f} ç§’ã€‚")
            
            return model_service_pb2.TtsResponse(generated_audio=wav_bytes)

        except Exception as e:
            print(f"âŒ è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(f"å…§éƒ¨ä¼ºæœå™¨éŒ¯èª¤: {str(e)}")
            return model_service_pb2.TtsResponse()
        
        finally:
            # --- æ¸…ç†æš«å­˜æª”æ¡ˆ ---
            if temp_file_path and os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
                print(f"ğŸ—‘ï¸ å·²åˆªé™¤æš«å­˜æª”æ¡ˆ: {temp_file_path}")