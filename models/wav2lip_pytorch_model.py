import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import cv2
import os
import sys

# =================== ç›´æ¥å®šç¾©å·ç©å±¤ ===================
class Conv2d(nn.Module):
    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.conv_block = nn.Sequential(
                            nn.Conv2d(cin, cout, kernel_size, stride, padding),
                            nn.BatchNorm2d(cout)
                            )
        self.act = nn.ReLU()
        self.residual = residual

    def forward(self, x):
        out = self.conv_block(x)
        if self.residual:
            out += x
        return self.act(out)

class nonorm_Conv2d(nn.Module):
    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.conv_block = nn.Sequential(
                            nn.Conv2d(cin, cout, kernel_size, stride, padding),
                            )
        self.act = nn.LeakyReLU(0.01, inplace=True)

    def forward(self, x):
        out = self.conv_block(x)
        return self.act(out)

class Conv2dTranspose(nn.Module):
    def __init__(self, cin, cout, kernel_size, stride, padding, output_padding=0, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.conv_block = nn.Sequential(
                            nn.ConvTranspose2d(cin, cout, kernel_size, stride, padding, output_padding),
                            nn.BatchNorm2d(cout)
                            )
        self.act = nn.ReLU()

    def forward(self, x):
        out = self.conv_block(x)
        return self.act(out)

# =================== ç›´æ¥å®šç¾© Wav2Lip æ¨¡å‹ ===================
class Wav2Lip(nn.Module):
    def __init__(self):
        super(Wav2Lip, self).__init__()

        self.face_encoder_blocks = nn.ModuleList([
            nn.Sequential(Conv2d(6, 16, kernel_size=7, stride=1, padding=3)), # 96,96

            nn.Sequential(Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # 48,48
            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True)),

            nn.Sequential(Conv2d(32, 64, kernel_size=3, stride=2, padding=1),    # 24,24
            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True)),

            nn.Sequential(Conv2d(64, 128, kernel_size=3, stride=2, padding=1),   # 12,12
            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True)),

            nn.Sequential(Conv2d(128, 256, kernel_size=3, stride=2, padding=1),       # 6,6
            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True)),

            nn.Sequential(Conv2d(256, 512, kernel_size=3, stride=2, padding=1),     # 3,3
            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),),
            
            nn.Sequential(Conv2d(512, 512, kernel_size=3, stride=1, padding=0),     # 1, 1
            Conv2d(512, 512, kernel_size=1, stride=1, padding=0)),])

        self.audio_encoder = nn.Sequential(
            Conv2d(1, 32, kernel_size=3, stride=1, padding=1),
            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),

            Conv2d(32, 64, kernel_size=3, stride=(3, 1), padding=1),
            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),

            Conv2d(64, 128, kernel_size=3, stride=3, padding=1),
            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),

            Conv2d(128, 256, kernel_size=3, stride=(3, 2), padding=1),
            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),

            Conv2d(256, 512, kernel_size=3, stride=1, padding=0),
            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)

        self.face_decoder_blocks = nn.ModuleList([
            nn.Sequential(Conv2d(512, 512, kernel_size=1, stride=1, padding=0),),

            nn.Sequential(Conv2dTranspose(1024, 512, kernel_size=3, stride=1, padding=0), # 3,3
            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),),

            nn.Sequential(Conv2dTranspose(1024, 512, kernel_size=3, stride=2, padding=1, output_padding=1),
            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),), # 6, 6

            nn.Sequential(Conv2dTranspose(768, 384, kernel_size=3, stride=2, padding=1, output_padding=1),
            Conv2d(384, 384, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(384, 384, kernel_size=3, stride=1, padding=1, residual=True),), # 12, 12

            nn.Sequential(Conv2dTranspose(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),
            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),), # 24, 24

            nn.Sequential(Conv2dTranspose(320, 128, kernel_size=3, stride=2, padding=1, output_padding=1), 
            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),), # 48, 48

            nn.Sequential(Conv2dTranspose(160, 64, kernel_size=3, stride=2, padding=1, output_padding=1),
            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),),]) # 96,96

        self.output_block = nn.Sequential(Conv2d(80, 32, kernel_size=3, stride=1, padding=1),
            nn.Conv2d(32, 3, kernel_size=1, stride=1, padding=0),
            nn.Sigmoid()) 

    def forward(self, audio_sequences, face_sequences):
        # audio_sequences = (B, T, 1, 80, 16)
        B = audio_sequences.size(0)

        input_dim_size = len(face_sequences.size())
        if input_dim_size > 4:
            audio_sequences = torch.cat([audio_sequences[:, i] for i in range(audio_sequences.size(1))], dim=0)
            face_sequences = torch.cat([face_sequences[:, :, i] for i in range(face_sequences.size(2))], dim=0)

        audio_embedding = self.audio_encoder(audio_sequences) # B, 512, 1, 1

        feats = []
        x = face_sequences
        for f in self.face_encoder_blocks:
            x = f(x)
            feats.append(x)

        x = audio_embedding
        for f in self.face_decoder_blocks:
            x = f(x)
            try:
                x = torch.cat((x, feats[-1]), dim=1)
            except Exception as e:
                print(x.size())
                print(feats[-1].size())
                raise e
            
            feats.pop()

        x = self.output_block(x)

        if input_dim_size > 4:
            x = torch.split(x, B, dim=0) # [(B, C, H, W)]
            outputs = torch.stack(x, dim=2) # (B, C, T, H, W)

        else:
            outputs = x
            
        return outputs

# å°‡ Wav2Lip è¨­ç½®ç‚ºä¸»è¦æ¨¡å‹
Wav2LipModel = Wav2Lip

# å°å…¥å…¶ä»–æ¨¡çµ„
try:
    import face_detection
    print("âœ… äººè‡‰æª¢æ¸¬æ¨¡çµ„å°å…¥æˆåŠŸ")
except ImportError:
    print("âš ï¸ äººè‡‰æª¢æ¸¬æ¨¡çµ„å°å…¥å¤±æ•—ï¼Œå°‡ä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ")
    face_detection = None

try:
    # å˜—è©¦å°å…¥ Wav2Lip éŸ³è¨Šæ¨¡çµ„ï¼ˆå¦‚æœå­˜åœ¨çš„è©±ï¼‰
    import audio
    print("âœ… éŸ³è¨Šè™•ç†æ¨¡çµ„å°å…¥æˆåŠŸ")
except ImportError:
    print("âš ï¸ éŸ³è¨Šè™•ç†æ¨¡çµ„å°å…¥å¤±æ•—ï¼Œå°‡ä½¿ç”¨ librosa")
    audio = None

print("âœ… Wav2Lip æ¨¡çµ„è¼‰å…¥æˆåŠŸ (è‡ªåŒ…å«ç‰ˆæœ¬)")

class Wav2LipPytorch:
    """ä½¿ç”¨ PyTorch ç‰ˆæœ¬çš„ Wav2Lip æ¨¡å‹"""
    
    def __init__(self, checkpoint_path="models/wav2lip_gan.pth"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self.face_detector = None
        self.checkpoint_path = checkpoint_path
        self.img_size = 96
        
        # é è¨­åƒæ•¸
        self.pads = [0, 10, 0, 0]  # top, bottom, left, right padding
        self.resize_factor = 1
        self.face_det_batch_size = 16
        self.wav2lip_batch_size = 128
        
        print(f"ğŸš€ åˆå§‹åŒ– Wav2Lip PyTorch æ¨¡å‹ï¼Œè¨­å‚™: {self.device}")
        
    def load_model(self):
        """è¼‰å…¥ Wav2Lip æ¨¡å‹"""
        if self.model is not None:
            return
            
        print(f"â³ è¼‰å…¥ Wav2Lip æ¨¡å‹å¾: {self.checkpoint_path}")
        
        if not os.path.exists(self.checkpoint_path):
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ: {self.checkpoint_path}")
        
        # è¼‰å…¥æ¨¡å‹
        self.model = Wav2LipModel()
        
        # è¼‰å…¥æ¬Šé‡
        checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
        
        # è™•ç†ä¸åŒçš„ checkpoint æ ¼å¼
        if "state_dict" in checkpoint:
            state_dict = checkpoint["state_dict"]
        elif "model" in checkpoint:
            state_dict = checkpoint["model"]
        else:
            state_dict = checkpoint
            
        # è¼‰å…¥æ¬Šé‡
        self.model.load_state_dict(state_dict)
        self.model.to(self.device)
        self.model.eval()
        
        print("âœ… Wav2Lip PyTorch æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        
    def load_face_detector(self):
        """è¼‰å…¥äººè‡‰æª¢æ¸¬å™¨"""
        if self.face_detector is not None:
            return
            
        print("â³ è¼‰å…¥äººè‡‰æª¢æ¸¬å™¨...")
        try:
            if face_detection is not None:
                self.face_detector = face_detection.FaceAlignment(
                    face_detection.LandmarksType._2D, 
                    flip_input=False, 
                    device=self.device
                )
                print("âœ… äººè‡‰æª¢æ¸¬å™¨è¼‰å…¥æˆåŠŸ")
            else:
                print("âš ï¸ äººè‡‰æª¢æ¸¬æ¨¡çµ„ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨ OpenCV ä½œç‚ºå‚™ç”¨æ–¹æ¡ˆ")
                import cv2
                cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
                self.face_detector = cv2.CascadeClassifier(cascade_path)
                print("âœ… OpenCV äººè‡‰æª¢æ¸¬å™¨è¼‰å…¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ äººè‡‰æª¢æ¸¬å™¨è¼‰å…¥å¤±æ•—: {e}")
            print("å°‡ä½¿ç”¨ OpenCV ä½œç‚ºå‚™ç”¨æ–¹æ¡ˆ")
            import cv2
            cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            self.face_detector = cv2.CascadeClassifier(cascade_path)
            
    def detect_faces(self, images):
        """æª¢æ¸¬äººè‡‰"""
        if self.face_detector is None:
            self.load_face_detector()
            
        # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨ face_detection æ¨¡çµ„é‚„æ˜¯ OpenCV
        if hasattr(self.face_detector, 'get_detections_for_batch'):
            # ä½¿ç”¨ face_detection æ¨¡çµ„
            return self._detect_faces_with_face_detection(images)
        else:
            # ä½¿ç”¨ OpenCV
            return self._detect_faces_with_opencv(images)
    
    def _detect_faces_with_face_detection(self, images):
        """ä½¿ç”¨ face_detection æ¨¡çµ„æª¢æ¸¬äººè‡‰"""
        batch_size = self.face_det_batch_size
        
        while True:
            predictions = []
            try:
                for i in range(0, len(images), batch_size):
                    batch = np.array(images[i:i + batch_size])
                    predictions.extend(self.face_detector.get_detections_for_batch(batch))
                break
            except RuntimeError as e:
                if batch_size == 1:
                    raise RuntimeError('åœ–ç‰‡å¤ªå¤§ç„¡æ³•é€²è¡Œäººè‡‰æª¢æ¸¬ã€‚è«‹ä½¿ç”¨ --resize_factor åƒæ•¸')
                batch_size //= 2
                print(f'å¾ OOM éŒ¯èª¤æ¢å¾©ï¼›æ–°æ‰¹æ¬¡å¤§å°: {batch_size}')
                continue
                
        # è™•ç†æª¢æ¸¬çµæœ
        results = []
        pady1, pady2, padx1, padx2 = self.pads
        
        for rect, image in zip(predictions, images):
            if rect is None:
                # å¦‚æœæ²’æœ‰æª¢æ¸¬åˆ°äººè‡‰ï¼Œä½¿ç”¨æ•´å€‹åœ–ç‰‡
                print("âš ï¸ æœªæª¢æ¸¬åˆ°äººè‡‰ï¼Œä½¿ç”¨æ•´å€‹åœ–ç‰‡")
                results.append([0, 0, image.shape[1], image.shape[0]])
                continue
                
            # ç¢ºä¿åº§æ¨™æ˜¯æ•´æ•¸
            y1 = max(0, int(rect[1] - pady1))
            y2 = min(image.shape[0], int(rect[3] + pady2))
            x1 = max(0, int(rect[0] - padx1))
            x2 = min(image.shape[1], int(rect[2] + padx2))
            
            results.append([x1, y1, x2, y2])
            
        return self.get_smoothened_boxes(results)
    
    def _detect_faces_with_opencv(self, images):
        """ä½¿ç”¨ OpenCV æª¢æ¸¬äººè‡‰"""
        results = []
        pady1, pady2, padx1, padx2 = self.pads
        
        for image in images:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            faces = self.face_detector.detectMultiScale(gray, 1.1, 4)
            
            if len(faces) > 0:
                x, y, w, h = faces[0]
                # ç¢ºä¿åº§æ¨™æ˜¯æ•´æ•¸
                y1 = max(0, int(y + pady1))
                y2 = min(image.shape[0], int(y + h + pady2))
                x1 = max(0, int(x + padx1))
                x2 = min(image.shape[1], int(x + w + padx2))
                results.append([x1, y1, x2, y2])
            else:
                print("âš ï¸ æœªæª¢æ¸¬åˆ°äººè‡‰ï¼Œä½¿ç”¨æ•´å€‹åœ–ç‰‡")
                results.append([0, 0, image.shape[1], image.shape[0]])
                
        return self.get_smoothened_boxes(results)
        
    def get_smoothened_boxes(self, boxes, T=5):
        """å¹³æ»‘åŒ–é‚Šç•Œæ¡†"""
        for i in range(len(boxes)):
            if i + T > len(boxes):
                window = boxes[len(boxes) - T:]
            else:
                window = boxes[i : i + T]
            boxes[i] = np.mean(window, axis=0)
        # ç¢ºä¿æ‰€æœ‰åº§æ¨™éƒ½æ˜¯æ•´æ•¸
        for i in range(len(boxes)):
            boxes[i] = [int(x) for x in boxes[i]]
        return boxes
        
    def datagen(self, frames, mels, face_coords):
        """ç”Ÿæˆæ¨¡å‹è¼¸å…¥æ•¸æ“š"""
        img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []
        
        for i, (frame, mel, coord) in enumerate(zip(frames, mels, face_coords)):
            # è£å‰ªäººè‡‰å€åŸŸ - ç¢ºä¿åº§æ¨™æ˜¯æ•´æ•¸
            x1, y1, x2, y2 = [int(c) for c in coord]
            face = frame[y1:y2, x1:x2]
            face = cv2.resize(face, (self.img_size, self.img_size))
            
            img_batch.append(face)
            mel_batch.append(mel)
            frame_batch.append(frame)
            coords_batch.append(coord)
            
            if len(img_batch) >= self.wav2lip_batch_size:
                img_batch = np.asarray(img_batch)
                mel_batch = np.asarray(mel_batch)
                
                # è½‰æ›ç‚ºæ¨¡å‹è¼¸å…¥æ ¼å¼
                img_masked = img_batch.copy()
                img_masked[:, self.img_size//2:] = 0
                
                img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.
                mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])
                
                yield img_batch, mel_batch, frame_batch, coords_batch
                img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []
                
        if len(img_batch) > 0:
            img_batch = np.asarray(img_batch)
            mel_batch = np.asarray(mel_batch)
            
            img_masked = img_batch.copy()
            img_masked[:, self.img_size//2:] = 0
            
            img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.
            mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])
            
            yield img_batch, mel_batch, frame_batch, coords_batch
            
    def inference(self, image_path, audio_path, output_path):
        """åŸ·è¡Œ Wav2Lip æ¨ç†"""
        if self.model is None:
            self.load_model()
            
        print("ğŸ“· è®€å–åœ–ç‰‡...")
        
        # è®€å–åœ–ç‰‡
        if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):
            frame = cv2.imread(image_path)
            if frame is None:
                raise ValueError(f"ç„¡æ³•è®€å–åœ–ç‰‡: {image_path}")
            frames = [frame]
        else:
            raise ValueError("ç›®å‰åªæ”¯æ´éœæ…‹åœ–ç‰‡")
            
        print("ğŸµ è™•ç†éŸ³è¨Š...")
        
        # è™•ç†éŸ³è¨Š - å„ªå…ˆä½¿ç”¨ Wav2Lip éŸ³è¨Šè™•ç†ï¼Œå¤±æ•—å‰‡ä½¿ç”¨ librosa
        try:
            if audio is not None:
                wav = audio.load_wav(audio_path, 16000)
                mel = audio.melspectrogram(wav)  # åªå‚³éä¸€å€‹åƒæ•¸
                
                if np.isnan(mel.reshape(-1)).sum() > 0:
                    raise ValueError('éŸ³è¨Šæª”æ¡ˆä¸­åŒ…å« NaN å€¼')
                    
                print(f"âœ… ä½¿ç”¨ Wav2Lip éŸ³è¨Šè™•ç†æˆåŠŸï¼Œmel å½¢ç‹€: {mel.shape}")
            else:
                raise ImportError("Wav2Lip éŸ³è¨Šæ¨¡çµ„ä¸å¯ç”¨")
                
        except Exception as e:
            print(f"âŒ ä½¿ç”¨ Wav2Lip éŸ³è¨Šè™•ç†å¤±æ•—: {e}")
            print("å˜—è©¦ä½¿ç”¨ librosa ä½œç‚ºå‚™ç”¨æ–¹æ¡ˆ...")
            
            # å‚™ç”¨éŸ³è¨Šè™•ç†æ–¹æ¡ˆ
            import librosa
            wav, sr = librosa.load(audio_path, sr=16000)
            mel = librosa.feature.melspectrogram(y=wav, sr=16000, n_mels=80, fmax=8000)
            mel = np.log(mel + 1e-6)
            print(f"âœ… ä½¿ç”¨ librosa éŸ³è¨Šè™•ç†æˆåŠŸï¼Œmel å½¢ç‹€: {mel.shape}")
            
        mel_chunks = []
        mel_idx_multiplier = 80./25. 
        i = 0
        while True:
            start_idx = int(i * mel_idx_multiplier)
            if start_idx + 16 < len(mel[0]):
                mel_chunks.append(mel[:, start_idx : start_idx + 16])
            else:
                break
            i += 1
            
        print(f"ğŸ“Š éŸ³è¨Šé•·åº¦: {len(mel_chunks)} å¹€")
        
        # æ“´å±•å¹€ä»¥åŒ¹é…éŸ³è¨Šé•·åº¦
        full_frames = frames * len(mel_chunks)
        
        print("ğŸ¯ æª¢æ¸¬äººè‡‰...")
        
        # æª¢æ¸¬äººè‡‰
        try:
            face_coords = self.detect_faces(full_frames)
        except Exception as e:
            print(f"âš ï¸ äººè‡‰æª¢æ¸¬å¤±æ•—: {e}")
            print("ä½¿ç”¨æ•´å€‹åœ–ç‰‡ä½œç‚ºäººè‡‰å€åŸŸ")
            # å¦‚æœäººè‡‰æª¢æ¸¬å¤±æ•—ï¼Œä½¿ç”¨æ•´å€‹åœ–ç‰‡
            h, w = frames[0].shape[:2]
            face_coords = [[0, 0, w, h]] * len(full_frames)
        
        print("ğŸ¤– åŸ·è¡Œ Wav2Lip æ¨ç†...")
        
        # å‰µå»ºå½±ç‰‡å¯«å…¥å™¨
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, 25, (frames[0].shape[1], frames[0].shape[0]))
        
        # åŸ·è¡Œæ¨ç†
        for img_batch, mel_batch, frame_batch, coords_batch in self.datagen(full_frames, mel_chunks, face_coords):
            with torch.no_grad():
                img_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(self.device)
                mel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(self.device)
                
                pred = self.model(mel_batch, img_batch)
                pred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.
                
                for p, f, coord in zip(pred, frame_batch, coords_batch):
                    # ç¢ºä¿åº§æ¨™æ˜¯æ•´æ•¸
                    x1, y1, x2, y2 = [int(c) for c in coord]
                    p = cv2.resize(p.astype(np.uint8), (x2 - x1, y2 - y1))
                    
                    f[y1:y2, x1:x2] = p
                    out.write(f)
                    
        out.release()
        print(f"âœ… å½±ç‰‡ç”Ÿæˆå®Œæˆ: {output_path}")
        
        return output_path
