import grpc
import logging
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoConfig, pipeline
import torch
from proto import model_service_pb2, model_service_pb2_grpc

logger = logging.getLogger(__name__)

class LLMServicer(model_service_pb2_grpc.MediaServiceServicer):
    def __init__(self, model_name="microsoft/DialoGPT-medium"):
        """
        åˆå§‹åŒ– LLM æœå‹™
        
        Args:
            model_name: Hugging Face æ¨¡å‹åç¨±
            å¸¸ç”¨é¸é …:
            - "microsoft/DialoGPT-medium" (å°è©±æ¨¡å‹)
            - "gpt2" (é€šç”¨æ–‡æœ¬ç”Ÿæˆ)
            - "facebook/blenderbot-400M-distill" (èŠå¤©æ©Ÿå™¨äºº)
            - "google/flan-t5-base" (æŒ‡ä»¤è·Ÿéš¨æ¨¡å‹)
        """
        self.model_name = model_name
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        print(f"ğŸ¤– æ­£åœ¨è¼‰å…¥ LLM æ¨¡å‹: {model_name}")
        print(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {self.device}")
        
        try:
            # è¼‰å…¥ tokenizer å’Œæ¨¡å‹é…ç½®
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            config = AutoConfig.from_pretrained(model_name)

            # æ ¹æ“šæ¨¡å‹é¡å‹é¸æ“‡å°æ‡‰çš„ AutoModel å’Œ pipeline ä»»å‹™
            if getattr(config, "is_encoder_decoder", False):
                ModelClass = AutoModelForSeq2SeqLM
                pipeline_task = "text2text-generation"
                self.model_type = "seq2seq"
                print("ğŸ” æª¢æ¸¬åˆ° Seq2Seq æ¨¡å‹æ¶æ§‹")
            else:
                ModelClass = AutoModelForCausalLM
                pipeline_task = "text-generation"
                self.model_type = "causal"
                print("ğŸ” æª¢æ¸¬åˆ° Causal LM æ¨¡å‹æ¶æ§‹")

            self.model = ModelClass.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
                device_map="auto" if self.device.type == "cuda" else None
            )
            
            # è¨­ç½® padding tokenï¼ˆå¦‚æœæ²’æœ‰çš„è©±ï¼‰
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # å‰µå»º pipeline
            self.generator = pipeline(
                pipeline_task,
                model=self.model,
                tokenizer=self.tokenizer,
                device=0 if self.device.type == "cuda" else -1,
                torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32
            )
            
            print("âœ… LLM æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ LLM æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise

    def GenerateText(self, request, context):
        """è™•ç†æ–‡æœ¬ç”Ÿæˆè«‹æ±‚"""
        try:
            logger.info(f"æ”¶åˆ°æ–‡æœ¬ç”Ÿæˆè«‹æ±‚: {request.prompt[:50]}...")
            
            # ç”Ÿæˆåƒæ•¸
            generation_config = {
                "max_new_tokens": request.max_tokens if request.max_tokens > 0 else 100,
                "temperature": request.temperature if request.temperature > 0 else 0.7,
                "top_p": request.top_p if request.top_p > 0 else 0.9,
                "do_sample": True,
                "pad_token_id": self.tokenizer.eos_token_id,
                "return_full_text": False
            }
            
            logger.info(f"ç”Ÿæˆåƒæ•¸: {generation_config}")
            
            # åŸ·è¡Œæ–‡æœ¬ç”Ÿæˆ
            outputs = self.generator(
                request.prompt,
                **generation_config
            )
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬
            generated_text = outputs[0]["generated_text"]
            
            logger.info(f"ç”Ÿæˆå®Œæˆï¼Œé•·åº¦: {len(generated_text)} å­—ç¬¦")
            
            return model_service_pb2.TextGenerationResponse(
                generated_text=generated_text,
                success=True
            )
            
        except Exception as e:
            error_msg = f"æ–‡æœ¬ç”Ÿæˆå¤±æ•—: {str(e)}"
            logger.error(error_msg)
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(error_msg)
            return model_service_pb2.TextGenerationResponse(
                generated_text="",
                success=False
            )

    def ChatCompletion(self, request, context):
        """è™•ç†å°è©±å®Œæˆè«‹æ±‚"""
        try:
            logger.info("æ”¶åˆ°å°è©±å®Œæˆè«‹æ±‚")
            
            # æ§‹å»ºå°è©±æ­·å²ç‚ºå–®ä¸€å­—ç¬¦ä¸²
            conversation_parts = []
            
            for message in request.messages:
                if message.role == "system":
                    conversation_parts.append(f"System: {message.content}")
                elif message.role == "user":
                    conversation_parts.append(f"Human: {message.content}")
                elif message.role == "assistant":
                    conversation_parts.append(f"Assistant: {message.content}")
            
            # æ·»åŠ  Assistant æç¤ºè®“æ¨¡å‹å›æ‡‰
            conversation_parts.append("Assistant:")
            conversation = "\n".join(conversation_parts)
            
            logger.info(f"å°è©±æ­·å²é•·åº¦: {len(conversation)} å­—ç¬¦")
            
            # ç”Ÿæˆå›æ‡‰
            generation_config = {
                "max_new_tokens": request.max_tokens if request.max_tokens > 0 else 150,
                "temperature": request.temperature if request.temperature > 0 else 0.7,
                "top_p": 0.9,
                "do_sample": True,
                "pad_token_id": self.tokenizer.eos_token_id,
                "return_full_text": False,
                "num_return_sequences": 1
            }
            
            # ä½¿ç”¨ pipeline ç”Ÿæˆå›æ‡‰
            outputs = self.generator(
                conversation,
                **generation_config
            )
            
            response_text = outputs[0]["generated_text"].strip()
            
            # æ¸…ç†å›æ‡‰ï¼ˆç§»é™¤å¯èƒ½çš„åœæ­¢åºåˆ—ï¼‰
            stop_sequences = ["\nHuman:", "\nSystem:", "Human:", "System:", "\nuser:", "\nUser:"]
            for stop_seq in stop_sequences:
                if stop_seq in response_text:
                    response_text = response_text.split(stop_seq)[0].strip()
            
            logger.info(f"å°è©±å›æ‡‰ç”Ÿæˆå®Œæˆ")
            
            return model_service_pb2.ChatCompletionResponse(
                response=response_text,
                success=True
            )
            
        except Exception as e:
            error_msg = f"å°è©±å®Œæˆå¤±æ•—: {str(e)}"
            logger.error(error_msg)
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(error_msg)
            return model_service_pb2.ChatCompletionResponse(
                response="",
                success=False
            )